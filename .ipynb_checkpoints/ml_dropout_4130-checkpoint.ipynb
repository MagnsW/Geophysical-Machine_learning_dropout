{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning - Prediction of Legal or Illegal Dropouts\n",
    "=====\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing modules for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting some parameters for reading the input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the header of two dan-files (broadband and conventional/cross-correlation) are listed to confirm the modeling parameters used in Nucleus. First specify the path to the dan files for the dataset to analyze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./batch_4130/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file = \"4130T__040_2000_080_5.dan\"\n",
    "f = open(path + \"bb_\" + file)\n",
    "lines = f.read().splitlines()\n",
    "f.close()\n",
    "for i, line in enumerate(lines):\n",
    "    if i < 56:\n",
    "        print(i, line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"4130T__040_2000_080_5.dan\"\n",
    "f = open(path + \"cc_\" + file)\n",
    "lines = f.read().splitlines()\n",
    "f.close()\n",
    "for i, line in enumerate(lines):\n",
    "    if i < 56:\n",
    "        print(i, line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the numbers of lines to skip. This is for the 4130 array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bb statistics\n",
    "onegunskip = 43\n",
    "twogunskip = 74\n",
    "spareskip = 539\n",
    "eof = 557\n",
    "\n",
    "#conventional statistics\n",
    "onegunskipcc = 55\n",
    "twogunskipcc = 86\n",
    "spareskipcc = 551\n",
    "eofcc = 569\n",
    "\n",
    "arrayvol = '4130T'\n",
    "arraydepth = [4, 5, 6, 7, 8, 9]\n",
    "subsep = [8, 10]\n",
    "temp = [5, 10, 15, 20, 25]\n",
    "prefix = ['bb', 'cc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "\n",
    "for a in arraydepth:\n",
    "\tfor s in subsep:\n",
    "\t\tfor t in temp:\n",
    "\t\t\tif s < 10: \n",
    "\t\t\t\tfilenames.append(arrayvol + '__0' + str(a) + '0_2000_0' + str(s) + '0_' + str(t) + '.dan')\n",
    "\t\t\telse:\n",
    "\t\t\t\tfilenames.append(arrayvol + '__0' + str(a) + '0_2000_' + str(s) + '0_' + str(t) + '.dan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just checking the file names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for filename in filenames:\n",
    "    if count < 10:\n",
    "        print(filename)\n",
    "    count += 1\n",
    "print(len(filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_one_gun_bb_raw = ['droparray1', 'dropgun1', 'gunvolume1', 'AvgdB', 'MaxdB', 'MaxPhase']\n",
    "columns_two_gun_bb_raw = ['droparray1', 'dropgun1', 'droparray2', 'dropgun2', 'gunvolume1', 'gunvolume2', 'AvgdB', 'MaxdB', 'MaxPhase']\n",
    "columns_spare_gun_bb_raw = ['droparray1', 'dropgun1', 'droparray2', 'dropgun2', 'gunvolume1', 'gunvolume2', 'AvgdB', 'MaxdB', 'MaxPhase']\t\n",
    "columns_one_gun_cc_raw = ['droparray1', 'dropgun1', 'gunvolume1', 'Peak', 'Peakch','PtoB','PtoBch', 'x-corr', 'AvgdB', 'MaxdB']\n",
    "columns_two_gun_cc_raw = ['droparray1', 'dropgun1', 'droparray2', 'dropgun2', 'gunvolume1', 'gunvolume2', 'Peak', 'Peakch', 'PtoB', 'PtoBch', 'x-corr', 'AvgdB', 'MaxdB']\n",
    "columns_spare_gun_cc_raw = ['droparray1', 'dropgun1', 'droparray2', 'dropgun2', 'gunvolume1', 'gunvolume2', 'Peak', 'Peakch', 'PtoB', 'PtoBch', 'x-corr', 'AvgdB', 'MaxdB']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical data defined as panda dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_one_gun_bb_raw = pd.DataFrame()\n",
    "stat_two_gun_bb_raw = pd.DataFrame()\n",
    "stat_spare_gun_bb_raw = pd.DataFrame()\n",
    "stat_one_gun_cc_raw = pd.DataFrame()\n",
    "stat_two_gun_cc_raw = pd.DataFrame()\n",
    "stat_spare_gun_cc_raw = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data from files into dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"./batch_4130/\"\n",
    "for filename in filenames:\n",
    "    temp_stat_one_gun_bb_raw = pd.read_csv(path + 'bb_' + filename, names=columns_one_gun_bb_raw, skiprows=onegunskip, delim_whitespace=True, nrows=twogunskip-onegunskip)\n",
    "    temp_stat_one_gun_bb_raw['filename'] = filename\n",
    "    temp_stat_one_gun_bb_raw['Depth'] = int(filename[7:10])\n",
    "    temp_stat_one_gun_bb_raw['Subsep'] = int(filename[16:19])\n",
    "    temp_stat_one_gun_bb_raw['Temperature'] = int(filename[20:22].replace('.',''))\n",
    "    stat_one_gun_bb_raw = stat_one_gun_bb_raw.append(temp_stat_one_gun_bb_raw)\n",
    "    temp_stat_one_gun_bb_raw = []\n",
    "    \n",
    "    temp_stat_two_gun_bb_raw = pd.read_csv(path + 'bb_' + filename, names=columns_two_gun_bb_raw, skiprows=twogunskip, delim_whitespace=True, nrows=spareskip-twogunskip)\n",
    "    temp_stat_two_gun_bb_raw['filename'] = filename\n",
    "    temp_stat_two_gun_bb_raw['Depth'] = int(filename[7:10])\n",
    "    temp_stat_two_gun_bb_raw['Subsep'] = int(filename[16:19])\n",
    "    temp_stat_two_gun_bb_raw['Temperature'] = int(filename[20:22].replace('.',''))\n",
    "    stat_two_gun_bb_raw = stat_two_gun_bb_raw.append(temp_stat_two_gun_bb_raw)\n",
    "    temp_stat_two_gun_bb_raw = []\n",
    "    \n",
    "    temp_stat_spare_gun_bb_raw = pd.read_csv(path + 'bb_' + filename, names=columns_spare_gun_bb_raw, skiprows=spareskip, delim_whitespace=True, nrows=eof-spareskip)\n",
    "    temp_stat_spare_gun_bb_raw['filename'] = filename\n",
    "    temp_stat_spare_gun_bb_raw['Depth'] = int(filename[7:10])\n",
    "    temp_stat_spare_gun_bb_raw['Subsep'] = int(filename[16:19])\n",
    "    temp_stat_spare_gun_bb_raw['Temperature'] = int(filename[20:22].replace('.',''))\n",
    "    stat_spare_gun_bb_raw = stat_spare_gun_bb_raw.append(temp_stat_spare_gun_bb_raw)\n",
    "    temp_stat_spare_gun_bb_raw = []\n",
    "    \n",
    "    temp_stat_one_gun_cc_raw = pd.read_csv(path + 'cc_' + filename, names=columns_one_gun_cc_raw, skiprows=onegunskipcc, delim_whitespace=True, nrows=twogunskipcc-onegunskipcc)\n",
    "    temp_stat_one_gun_cc_raw['filename'] = filename\n",
    "    temp_stat_one_gun_cc_raw['Depth'] = int(filename[7:10])\n",
    "    temp_stat_one_gun_cc_raw['Subsep'] = int(filename[16:19])\n",
    "    temp_stat_one_gun_cc_raw['Temperature'] = int(filename[20:22].replace('.',''))\n",
    "    stat_one_gun_cc_raw = stat_one_gun_cc_raw.append(temp_stat_one_gun_cc_raw)\n",
    "    temp_stat_one_gun_cc_raw = []\n",
    "    \n",
    "    temp_stat_two_gun_cc_raw = pd.read_csv(path + 'cc_' + filename, names=columns_two_gun_cc_raw, skiprows=twogunskipcc, delim_whitespace=True, nrows=spareskipcc-twogunskipcc)\n",
    "    temp_stat_two_gun_cc_raw['filename'] = filename\n",
    "    temp_stat_two_gun_cc_raw['Depth'] = int(filename[7:10])\n",
    "    temp_stat_two_gun_cc_raw['Subsep'] = int(filename[16:19])\n",
    "    temp_stat_two_gun_cc_raw['Temperature'] = int(filename[20:22].replace('.',''))\n",
    "    stat_two_gun_cc_raw = stat_two_gun_cc_raw.append(temp_stat_two_gun_cc_raw)\n",
    "    temp_stat_two_gun_cc_raw = []\n",
    "    \n",
    "    temp_stat_spare_gun_cc_raw = pd.read_csv(path + 'cc_' + filename, names=columns_spare_gun_cc_raw, skiprows=spareskipcc, delim_whitespace=True, nrows=eofcc-spareskipcc)\n",
    "    temp_stat_spare_gun_cc_raw['filename'] = filename\n",
    "    temp_stat_spare_gun_cc_raw['Depth'] = int(filename[7:10])\n",
    "    temp_stat_spare_gun_cc_raw['Subsep'] = int(filename[16:19])\n",
    "    temp_stat_spare_gun_cc_raw['Temperature'] = int(filename[20:22].replace('.',''))\n",
    "    stat_spare_gun_cc_raw = stat_spare_gun_cc_raw.append(temp_stat_spare_gun_cc_raw)\n",
    "    temp_stat_spare_gun_cc_raw = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The follwing is a lambda function that converts to string and adds zeros to a total length of 2 characters to the input x (which could be an int). Is later used to convert all gun numbers from int to string. (3 to \"03\", for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_zero = lambda x: str(int(x)).zfill(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding all the string representations for gun numbering in a dedicated set of columns. Is later used to make array_gun_number index with a logical order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_one_gun_bb_raw['dropgun1_str'] = stat_one_gun_bb_raw['dropgun1'].apply(add_zero)\n",
    "stat_two_gun_bb_raw['dropgun1_str'] = stat_two_gun_bb_raw['dropgun1'].apply(add_zero)\n",
    "stat_two_gun_bb_raw['dropgun2_str'] = stat_two_gun_bb_raw['dropgun2'].apply(add_zero)\n",
    "stat_spare_gun_bb_raw['dropgun1_str'] = stat_spare_gun_bb_raw['dropgun1'].apply(add_zero)\n",
    "stat_spare_gun_bb_raw['dropgun2_str'] = stat_spare_gun_bb_raw['dropgun2'].apply(add_zero)\n",
    "stat_one_gun_cc_raw['dropgun1_str'] = stat_one_gun_cc_raw['dropgun1'].apply(add_zero)\n",
    "stat_two_gun_cc_raw['dropgun1_str'] = stat_two_gun_cc_raw['dropgun1'].apply(add_zero)\n",
    "stat_two_gun_cc_raw['dropgun2_str'] = stat_two_gun_cc_raw['dropgun2'].apply(add_zero)\n",
    "stat_spare_gun_cc_raw['dropgun1_str'] = stat_spare_gun_bb_raw['dropgun1'].apply(add_zero)\n",
    "stat_spare_gun_cc_raw['dropgun2_str'] = stat_spare_gun_bb_raw['dropgun2'].apply(add_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing some statistics to check that dataframes are ok (Remove hash for what you want to print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print('stat_one_gun_bb_raw: ')\n",
    "#print(stat_one_gun_bb_raw.head())\n",
    "#print(stat_one_gun_bb_raw.head())\n",
    "#print(stat_one_gun_bb_raw.tail())\n",
    "#print(stat_one_gun_cc_raw)\n",
    "#print(stat_two_gun_bb_raw.info())\n",
    "#print('stat_spare_gun')\n",
    "\n",
    "#print(stat_one_gun_cc_raw.head())\n",
    "#print(stat_one_gun_cc_raw.tail())\n",
    "#print('stat_two_gun_cc_raw: ')\n",
    "#print(stat_two_gun_cc_raw.head())\n",
    "#print(stat_two_gun_cc_raw.info())\n",
    "# print('stat_spare_gun_cc_raw')\n",
    "print(stat_spare_gun_cc_raw.tail())\n",
    "# print(stat_spare_gun_cc_raw.info())\n",
    "#print(len(stat_one_gun_bb_raw), len(stat_two_gun_bb_raw), len(stat_spare_gun_bb_raw))\n",
    "#print(len(stat_one_gun_cc_raw), len(stat_two_gun_cc_raw), len(stat_spare_gun_cc_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to copy dataframes to identical copy with \"all\" postfix. Then later, the mastermatrix filter will be applied to the original dataframes. The \"all\" dataframes will contain the unfiltered data material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(stat_two_gun_bb_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_two_gun_bb_raw['legal'] = stat_two_gun_bb_raw.apply(lambda row: 1 if (row['AvgdB'] < 0.85) & \n",
    "                                                         (row['MaxdB'] < 3) & \n",
    "                                                         (row['MaxPhase'] < 20) \n",
    "                                                         else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_two_gun_bb_raw.legal.value_counts()\n",
    "# 1 means legal (55%), 0 means illegal (45%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_two_gun_bb_raw.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_two_gun_bb_raw['gun1_unique'] = stat_two_gun_bb_raw.apply(lambda row: str(row['droparray1']) + '.' + row['dropgun1_str'], axis=1)\n",
    "stat_two_gun_bb_raw['gun2_unique'] = stat_two_gun_bb_raw.apply(lambda row: str(row['droparray2']) + '.' + row['dropgun2_str'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stat_two_gun_bb_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data_two_gun_bb = stat_two_gun_bb_raw[['filename', 'gun1_unique', 'gun2_unique', 'Depth', 'Subsep', 'Temperature', 'legal']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ml_data_two_gun_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valuemapping(values_to_map):\n",
    "    values = values_to_map.unique()\n",
    "    mapping = {}\n",
    "    for i, value in enumerate(values, 1):\n",
    "        mapping[value] = i\n",
    "    return mapping  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gun_mapping = {'1.01': 1, \n",
    "               '1.02': 2, \n",
    "               '1.03': 3, \n",
    "               '1.04': 4, \n",
    "               '1.05': 5, \n",
    "               '1.07': 6, \n",
    "               '1.09': 7, \n",
    "               '1.11': 9, \n",
    "               '1.12': 10, \n",
    "               '1.13': 11, \n",
    "               '1.14': 12, \n",
    "               '2.01': 13, \n",
    "               '2.02': 14, \n",
    "               '2.03': 15, \n",
    "               '2.05': 16, \n",
    "               '2.07': 17, \n",
    "               '2.09': 18, \n",
    "               '2.11': 19, \n",
    "               '2.12': 20, \n",
    "               '2.13': 21, \n",
    "               '3.01': 23, \n",
    "               '3.02': 24, \n",
    "               '3.03': 25, \n",
    "               '3.05': 27, \n",
    "               '3.07': 28, \n",
    "               '3.09': 29, \n",
    "               '3.10': 30, \n",
    "               '3.11': 31, \n",
    "               '3.12': 32, \n",
    "               '3.13': 33, \n",
    "               '3.14': 34}\n",
    "#gun_mapping1 = valuemapping(ml_data_two_gun_bb.gun1_unique)\n",
    "#gun_mapping2 = valuemapping(ml_data_two_gun_bb.gun2_unique)\n",
    "print(gun_mapping)\n",
    "ml_data_two_gun_bb['gun1_num'] = ml_data_two_gun_bb.gun1_unique.map(gun_mapping)\n",
    "ml_data_two_gun_bb['gun2_num'] = ml_data_two_gun_bb.gun2_unique.map(gun_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data_two_gun_bb = ml_data_two_gun_bb[['filename', 'gun1_num', 'gun2_num', 'Depth', 'Subsep', 'Temperature', 'legal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(ml_data_two_gun_bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ml_data_two_gun_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotmatrix(x, y, legal, title):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    #color= ['red' if legal == 0 else 'green']\n",
    "    color= ['red' if l == 0 else 'green' for l in legal]\n",
    "    plt.scatter(x, y, c=color, s=100)\n",
    "    plt.xlim(0, 36)\n",
    "    plt.ylim(0, 36)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Gun 1\")\n",
    "    plt.ylabel(\"Gun 2\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    filename = title + \".png\"\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examplefile = '4130T__070_2000_100_25.dan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data_example = ml_data_two_gun_bb[(ml_data_two_gun_bb['filename'] == examplefile)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotmatrix(ml_data_example.gun1_num, ml_data_example.gun2_num, ml_data_example.legal, 'Original Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(feature_data):\n",
    "    x = feature_data.values.astype('float64')\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    feature_data = pd.DataFrame(x_scaled, columns=feature_data.columns)\n",
    "    return feature_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_custom(feature_data):\n",
    "    feature_data['Depth'] = feature_data.apply(lambda row: row['Depth']/10, axis=1)\n",
    "    feature_data['Subsep'] = feature_data.apply(lambda row: row['Subsep']/20, axis=1)\n",
    "    feature_data['Temperature'] = feature_data.apply(lambda row: row['Temperature']/5, axis=1)\n",
    "    return feature_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_custom_inverse(feature_data):\n",
    "    feature_data['Depth'] = feature_data.apply(lambda row: row['Depth']*10, axis=1)\n",
    "    feature_data['Subsep'] = feature_data.apply(lambda row: row['Subsep']*20, axis=1)\n",
    "    feature_data['Temperature'] = feature_data.apply(lambda row: row['Temperature']*5, axis=1)\n",
    "    return feature_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(training_data, training_labels, validation_data, validation_labels, maxk, avg):\n",
    "    normalize_custom(training_data)\n",
    "    normalize_custom(validation_data)\n",
    "    k_list = []\n",
    "    accuracies = []\n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    f1s = []\n",
    "    for k in range(1, maxk, 2):\n",
    "        classifier = KNeighborsClassifier(n_neighbors = k, p=1)\n",
    "        classifier.fit(training_data, training_labels)\n",
    "        print(k, ': ', classifier.score(validation_data, validation_labels))\n",
    "        k_list.append(k)\n",
    "        y_predict = classifier.predict(validation_data)\n",
    "        accuracies.append(metrics.accuracy_score(validation_labels, y_predict))\n",
    "        recalls.append(metrics.recall_score(validation_labels, y_predict, average=avg))\n",
    "        precisions.append(metrics.precision_score(validation_labels, y_predict, average=avg))\n",
    "        f1s.append(metrics.f1_score(validation_labels, y_predict, average=avg))\n",
    "    normalize_custom_inverse(training_data)\n",
    "    normalize_custom_inverse(validation_data)\n",
    "    return k_list, y_predict, accuracies, recalls, precisions, f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_features = ml_data_two_gun_bb\n",
    "prediction_labels = ml_data_two_gun_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data, training_labels, test_labels = train_test_split(prediction_features, prediction_labels, test_size = 0.2, random_state = 100)\n",
    "print(len(training_data), len(training_labels))\n",
    "print(len(test_data), len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_one_config = training_data[(training_data['filename'] == examplefile)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotmatrix(training_one_config.gun1_num, training_one_config.gun2_num, training_one_config.legal, 'Training Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_one_config = test_data[(test_data['filename'] == examplefile)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotmatrix(test_one_config.gun1_num, test_one_config.gun2_num, test_one_config.legal, 'Test Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.drop([\"legal\", \"filename\"], axis=1)\n",
    "test_data_with_filename = test_data.drop([\"legal\"], axis=1)\n",
    "test_data = test_data_with_filename.drop([\"filename\"], axis=1)\n",
    "training_labels = training_labels[\"legal\"]\n",
    "test_labels = test_labels[\"legal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list, prediction, accuracies, recalls, precisions, f1 = knn(training_data, training_labels, test_data, test_labels, 101, 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k=3 is the sweetspot without normalization\n",
    "k=5 is the sweetspot with costum normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotaccuracy(k_list, accuracies, recalls, precision, f1, title):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(k_list, accuracies, label='accuracy')\n",
    "    plt.plot(k_list, recalls, label='recall')\n",
    "    plt.plot(k_list, precision, label='precision')\n",
    "    plt.plot(k_list, f1, label='f1')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Validation Accuracy\")\n",
    "    plt.title(title)\n",
    "    filename = \"Accuracy.png\"\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotaccuracy(k_list, accuracies, recalls, precisions, f1, \"Legal/illegal Classifier Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples. The best value is 1 and the worst value is 0.\n",
    "\n",
    "The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The best value is 1 and the worst value is 0.\n",
    "\n",
    "The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
    "\n",
    "F1 = 2 (precision recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running classification and prediction with best k\n",
    "normalize_custom(training_data)\n",
    "normalize_custom(test_data)\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, p=1)\n",
    "classifier.fit(training_data, training_labels)\n",
    "print(classifier.score(test_data, test_labels))\n",
    "y_predict = classifier.predict(test_data)\n",
    "normalize_custom_inverse(training_data)\n",
    "normalize_custom_inverse(test_data);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running classification and prediction with best k\n",
    "#classifier = KNeighborsClassifier(n_neighbors = 3)\n",
    "#classifier.fit(training_data_norm, training_labels)\n",
    "#print(classifier.score(test_data_norm, test_labels))\n",
    "#y_predict = classifier.predict(test_data_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(test_labels, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [0, 1]\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "filename = \"Confusion Matrix.png\"\n",
    "plt.savefig(filename)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "filename = \"Confusion Matrix Norm.png\"\n",
    "plt.savefig(filename)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction1(list):\n",
    "    df = pd.DataFrame([list])\n",
    "    return classifier.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_matrix = test_data_with_filename\n",
    "predicted_matrix['legal'] = test_data.apply(lambda row: prediction1([row['gun1_num'], row['gun2_num'], row['Depth'], row['Subsep'], row['Temperature']]), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#predicted_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_matrix_example = predicted_matrix[(predicted_matrix['filename'] == examplefile)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotmatrix(test_one_config.gun1_num, test_one_config.gun2_num, test_one_config.legal, 'Test Matrix')\n",
    "\n",
    "plotmatrix(predicted_matrix_example.gun1_num, predicted_matrix_example.gun2_num, predicted_matrix_example.legal, 'Predicted Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotmatrix_combine(x1, y1, legal1, x2, y2, legal2, title):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    color1= ['red' if l == 0 else 'green' for l in legal1]\n",
    "    color2= ['red' if l == 0 else 'green' for l in legal2]\n",
    "    plt.scatter(x1, y1, c=color1, s=100)\n",
    "    plt.scatter(x2, y2, c=color2, s=100)\n",
    "    plt.xlim(0, 36)\n",
    "    plt.ylim(0, 36)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Gun 1\")\n",
    "    plt.ylabel(\"Gun 2\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    filename = \"Combined Matrix.png\"\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotmatrix_combine(training_one_config.gun1_num, training_one_config.gun2_num, training_one_config.legal, predicted_matrix_example.gun1_num, predicted_matrix_example.gun2_num, predicted_matrix_example.legal, 'Combined Matrix (Training + Prediction)')\n",
    "plotmatrix(ml_data_example.gun1_num, ml_data_example.gun2_num, ml_data_example.legal, 'Original Matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = [5, 15, 77, 90, 23]\n",
    "df = pd.DataFrame([test1])\n",
    "df = df.rename(columns={0: 'gun1_num', 1: 'gun2_num', 2: 'Depth', 3: 'Subsep', 4: 'Temperature'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_custom(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted answer: \", classifier.predict(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_custom_inverse(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
